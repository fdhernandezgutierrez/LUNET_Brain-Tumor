{"cells":[{"cell_type":"code","execution_count":null,"id":"f7052a44","metadata":{"id":"f7052a44"},"outputs":[],"source":["#!pip install -U tensorflow-addons\n","import tensorflow as tf\n","import tensorflow_addons as tfa\n","import numpy as np\n","import tensorflow.keras.backend as K\n","import matplotlib.pyplot as plt\n","import nibabel as nib\n","import glob\n","import os\n","import gc\n","from tensorflow.keras.callbacks import LearningRateScheduler\n","import numpy as np\n","import os\n","import sys\n","import random\n","from skimage.io import imread, imshow\n","from skimage.transform import resize\n","from sklearn.model_selection import train_test_split\n","import time\n","#import cv2 as cv\n","from tensorflow.keras.callbacks import *\n","from tensorflow.python.saved_model import loader_impl # lib for saving models\n","import plotly.graph_objects as go\n","#import seaborn as sns\n","#import pandas as pd\n","import random; random.seed(2)"]},{"cell_type":"code","execution_count":null,"id":"e45d5ea7","metadata":{"id":"e45d5ea7"},"outputs":[],"source":["class CustomCallback(tf.keras.callbacks.Callback):\n","    def __init__(self) -> None:\n","        super().__init__()\n","\n","    #ef on_train_batch_begin(self, batch, logs = None):\n","    #    #lr = k.get_values(self.model.optimizer.lr)\n","    #    lr = tf.keras.backend.get_value(\n","    #        self.model.optimizer.lr(self.model.optimizer.iterations))\n","    #    #print(self.model.optimizer._decayed_lr('float32').numpy())\n","    #    print('\\nLearning rate:',lr)\n","    #    #tf.summary.scalar('learning rate', data=lr, step=epoch)\n","\n","class printlearningrate(tf.keras.callbacks.Callback):\n","    def on_epoch_end(self, epoch, logs={}):\n","        optimizer = self.model.optimizer\n","        lr = 10#float(K.get_value(optimizer.lr))\n","        Epoch_count = epoch + 1\n","\n","        #print('\\n Learning Rate:',lr)\n","        #print('\\n', \"Epoch:\", Epoch_count, ', LR: {:.2f}'.format(lr))"]},{"cell_type":"markdown","id":"a45c8808","metadata":{"id":"a45c8808"},"source":["<h1><center> UNET Base </center></h1>"]},{"cell_type":"code","execution_count":null,"id":"548eafc0","metadata":{"id":"548eafc0"},"outputs":[],"source":["class UNET_BASE(CustomCallback, printlearningrate):\n","    def __init__(self, optimizer, activation_function, initializer, inputs, initial_filter =16):\n","        self.optimizer           = optimizer\n","        self.activation_function = activation_function\n","        self.initializer         = initializer\n","        self.initial_filter      = initial_filter\n","        self.inputs              = inputs\n","\n","    def Unet(self):\n","        inputs = self.inputs\n","        initializer    = self.initializer\n","        initial_filter = self.initial_filter\n","        activation_func = self.activation_function\n","        s  = tf.keras.layers.Lambda(lambda x: x)(inputs) # El maximo valor es 505\n","        c1 = tf.keras.layers.Conv2D(initial_filter, (3,3), activation=activation_func, kernel_initializer=initializer, padding='same', name='c1_1' )(s)\n","        c1 = tf.keras.layers.Dropout(0.1)(c1) #Original 0.1\n","        c1 = tf.keras.layers.Conv2D(initial_filter, (3,3), activation=activation_func, kernel_initializer=initializer, padding='same', name='c1_2' )(c1)\n","        p1 = tf.keras.layers.MaxPool2D((2,2))(c1)\n","\n","        c2 = tf.keras.layers.Conv2D(initial_filter*2, (3,3), activation=activation_func, kernel_initializer=initializer, padding='same', name='c2_1')(p1)\n","        c2 = tf.keras.layers.Dropout(0.1)(c2) #original 0.1\n","        ##c2 = tf.keras.layers.Dropout(general_dropOut)(c2) #Orignal 0.1\n","        c2 = tf.keras.layers.Conv2D(initial_filter*2, (3,3), activation=activation_func, kernel_initializer=initializer, padding='same', name='c2_2')(c2)\n","        p2 = tf.keras.layers.MaxPool2D((2,2))(c2)\n","\n","        c3 = tf.keras.layers.Conv2D(initial_filter*4, (3,3), activation=activation_func, kernel_initializer=initializer, padding='same', name='c3_1')(p2)\n","        c3 = tf.keras.layers.Dropout(0.2)(c3)#original 0.2\n","        c3 = tf.keras.layers.Conv2D(initial_filter*4, (3,3), activation=activation_func, kernel_initializer=initializer, padding='same', name='c3_2')(c3)\n","        p3 = tf.keras.layers.MaxPool2D((2,2))(c3)\n","\n","        c4 = tf.keras.layers.Conv2D(initial_filter*8, (3,3), activation=activation_func, kernel_initializer=initializer, padding='same', name='c4_1')(p3)\n","        c4 = tf.keras.layers.Dropout(0.2)(c4)#original 0.2\n","        c4 = tf.keras.layers.Conv2D(initial_filter*8, (3,3), activation=activation_func, kernel_initializer=initializer, padding='same', name='c4_2')(c4)\n","        p4 = tf.keras.layers.MaxPool2D((2,2))(c4)\n","\n","        c5 = tf.keras.layers.Conv2D(initial_filter*16, (3,3), activation=activation_func, kernel_initializer=initializer, padding='same', name='c5_1')(p4)\n","        c5 = tf.keras.layers.Dropout(0.3)(c5)#original 0.3\n","        c5 = tf.keras.layers.Conv2D(initial_filter*16, (3,3), activation=activation_func, kernel_initializer=initializer, padding='same', name='c5_2')(c5)\n","\n","        #space latent\n","        u6 = tf.keras.layers.Conv2DTranspose(initial_filter*8, (2,2), strides=(2,2), padding='same', name='u6')(c5)\n","        u6 = tf.keras.layers.concatenate([u6, c4])\n","        c6 = tf.keras.layers.Conv2D(initial_filter*8, (3,3), activation=activation_func, kernel_initializer=initializer, padding='same', name='c6_1')(u6)\n","        c6 = tf.keras.layers.Dropout(0.2)(c6) #original 0.2\n","        c6 = tf.keras.layers.Conv2D(initial_filter*8, (3,3), activation=activation_func, kernel_initializer=initializer, padding='same', name='c6_2')(c6)\n","\n","        u7 = tf.keras.layers.Conv2DTranspose(initial_filter*4, (2,2), strides=(2,2), padding='same')(c6)\n","        u7 = tf.keras.layers.concatenate([u7, c3])\n","        c7 = tf.keras.layers.Conv2D(initial_filter*4, (3,3), activation=activation_func, kernel_initializer=initializer, padding='same')(u7)\n","        c7 = tf.keras.layers.Dropout(0.2)(c7)# original 0.2\n","        c7 = tf.keras.layers.Conv2D(initial_filter*4, (3,3), activation=activation_func, kernel_initializer=initializer, padding='same')(c7)\n","\n","        u8 = tf.keras.layers.Conv2DTranspose(initial_filter*2, (2,2), strides=(2,2), padding='same')(c7)\n","        u8 = tf.keras.layers.concatenate([u8, c2])\n","        c8 = tf.keras.layers.Conv2D(initial_filter*2, (3,3), activation=activation_func, kernel_initializer=initializer, padding='same')(u8)\n","        c8 = tf.keras.layers.Dropout(0.1)(c8) #original 0.1\n","        c8 = tf.keras.layers.Conv2D(initial_filter*2, (3,3), activation=activation_func, kernel_initializer=initializer, padding='same')(c8)\n","\n","        u9 = tf.keras.layers.Conv2DTranspose(initial_filter, (2,2), strides=(2,2), padding='same')(c8)\n","        u9 = tf.keras.layers.concatenate([u9, c1], axis=3)\n","        c9 = tf.keras.layers.Conv2D(initial_filter, (3,3), activation=activation_func, kernel_initializer=initializer, padding='same')(u9)\n","        c9 = tf.keras.layers.Dropout(0.1)(c9)#original 0.1\n","        c9 = tf.keras.layers.Conv2D(initial_filter, (3,3), activation=activation_func, kernel_initializer=initializer, padding='same')(c9)\n","\n","        outputs = tf.keras.layers.Conv2D(1, (1,1), activation='sigmoid')(c9)\n","        Model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n","        return Model\n","\n","\n","\n","    def train(self, X_train_, Y_train_, btz, epochs, min_lr = 0.001, max_lr = 0.8):\n","\n","        INIT_LR    = min_lr #1e-6  0.001\n","        MAX_LR     = max_lr   #1e-4\n","        steps_per_epoch = len(X_train_) // btz\n","        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","                                    filepath          = 'test',\n","                                    verbose           = True,\n","                                    save_weights_only = True,\n","                                    monitor           = 'val_loss',\n","                                    mode              = 'max',\n","                                    save_best_only=True)\n","        self.Model = self.Unet()\n","        self.Model.compile(optimizer=self.optimizer, loss='binary_crossentropy',\n","                           metrics=['accuracy'])\n","        results = self.Model.fit(X_train_,Y_train_, validation_split=0.20, batch_size=btz,\n","                                 epochs = epochs,\n","                                 callbacks=[model_checkpoint_callback,CustomCallback(), printlearningrate()])\n","\n","    def testing(self, X_test_, Y_test_, th = 0.5):\n","        metrics = []\n","        self.iou_score   = 0\n","        self.dc          = 0.0\n","        self.sensitivity = 0\n","        self.specificity = 0\n","        SM          = Semantic_loss_functions()\n","        #print(np.shape(Y_test_))\n","        for i in np.arange(len(X_test_)):\n","            img_  = resize(X_test_[i], (240, 240), mode='constant', preserve_range=True)\n","            if np.min(X_test_[i]) == np.max(X_test_[i]):\n","                continue\n","            img_ = np.expand_dims(img_, axis=0)\n","            y_p  = self.Model.predict(img_)\n","            im_p = (y_p>= th).astype(np.uint8)\n","            im_p = np.squeeze(im_p)\n","            y_test = np.uint8(Y_test_[i])\n","            intersection = np.logical_and( y_test, im_p)\n","            union = np.logical_or(y_test, im_p)\n","            #print('Interseccion',np.sum(intersection))\n","            if np.sum(union) > 0:\n","                self.iou_score += np.sum(intersection) / ( np.sum(union))\n","            else:\n","                self.iou_score += 0\n","            #print('np.sum(union)',np.sum(union))\n","\n","            im_p         = im_p.astype(np.float32)\n","            y_test       = y_test.astype(np.float32)\n","            self.dc          += tf.get_static_value(SM.dice_coef(y_test, im_p) )\n","            self.sensitivity += tf.get_static_value(SM.sensitivity(y_test, im_p ))\n","            self.specificity += tf.get_static_value(SM.specificity(y_test, im_p ))\n","\n","            #iou_score= iou_score/len(X_test_)\n","        self.IOU         = np.float32(self.iou_score/(len(X_test_)))\n","        self.dc          = np.float32( self.dc /(len(X_test_)))\n","        self.sensitivity = self.sensitivity / (len(X_test_))\n","        self.specificity =  self.specificity / (len(X_test_))\n","        #print('IOU score:', self.iou_score/(len(X_test_)))\n","        metrics.append(self.IOU)\n","        metrics.append(self.dc)\n","        metrics.append(self.sensitivity)\n","        metrics.append(self.specificity)\n","        return metrics\n","\n","    def get_model(self):\n","        return self.Model"]},{"cell_type":"code","execution_count":null,"id":"363fff29","metadata":{"id":"363fff29"},"outputs":[],"source":["class UNET(CustomCallback, printlearningrate):\n","    def __init__(self, optimizer, activation_function, initializer, inputs, initial_filter =16):\n","        self.optimizer           = optimizer\n","        self.activation_function = activation_function\n","        self.initializer         = initializer\n","        self.initial_filter      = initial_filter\n","        self.inputs              = inputs\n","        self.history = 0\n","\n","    def Unet(self):\n","        inputs = self.inputs\n","        initializer    = self.initializer\n","        initial_filter = self.initial_filter\n","        activation_func = self.activation_function\n","        s  = tf.keras.layers.Lambda(lambda x: x)(inputs) # El maximo valor es 505\n","        c1 = tf.keras.layers.Conv2D(initial_filter, (3,3), activation=activation_func, kernel_initializer=initializer, padding='same', name='c1_1' )(s)\n","        c1 = tf.keras.layers.BatchNormalization()(c1)\n","        c1 = tf.keras.layers.Dropout(0.1)(c1) #Original 0.1\n","        c1 = tf.keras.layers.Conv2D(initial_filter, (3,3), activation=activation_func, kernel_initializer=initializer, padding='same', name='c1_2' )(c1)\n","        c1 = tf.keras.layers.BatchNormalization()(c1)\n","        p1 = tf.keras.layers.MaxPool2D((2,2))(c1)\n","\n","        c2 = tf.keras.layers.Conv2D(initial_filter*2, (3,3), activation=activation_func, kernel_initializer=initializer, padding='same', name='c2_1')(p1)\n","        c2 = tf.keras.layers.BatchNormalization()(c2)\n","        c2 = tf.keras.layers.Dropout(0.1)(c2) #original 0.1\n","        c2 = tf.keras.layers.Conv2D(initial_filter*2, (3,3), activation=activation_func, kernel_initializer=initializer, padding='same', name='c2_2')(c2)\n","        c2 = tf.keras.layers.BatchNormalization()(c2)\n","        p2 = tf.keras.layers.MaxPool2D((2,2))(c2)\n","\n","        c3 = tf.keras.layers.Conv2D(initial_filter*4, (3,3), activation=activation_func, kernel_initializer=initializer, padding='same', name='c3_1')(p2)\n","        c3 = tf.keras.layers.BatchNormalization()(c3)\n","        c3 = tf.keras.layers.Dropout(0.2)(c3)#original 0.2\n","        c3 = tf.keras.layers.Conv2D(initial_filter*4, (3,3), activation=activation_func, kernel_initializer=initializer, padding='same', name='c3_2')(c3)\n","        c3 = tf.keras.layers.BatchNormalization()(c3)\n","        p3 = tf.keras.layers.MaxPool2D((2,2))(c3)\n","\n","        c4 = tf.keras.layers.Conv2D(initial_filter*8, (3,3), activation=activation_func, kernel_initializer=initializer, padding='same', name='c4_1')(p3)\n","        c4 = tf.keras.layers.BatchNormalization()(c4)\n","        c4 = tf.keras.layers.Dropout(0.2)(c4)#original 0.2\n","        c4 = tf.keras.layers.Conv2D(initial_filter*8, (3,3), activation=activation_func, kernel_initializer=initializer, padding='same', name='c4_2')(c4)\n","        c4 = tf.keras.layers.BatchNormalization()(c4)\n","        p4 = tf.keras.layers.MaxPool2D((2,2))(c4)\n","\n","        c5 = tf.keras.layers.Conv2D(initial_filter*16, (3,3), activation=activation_func, kernel_initializer=initializer, padding='same', name='c5_1')(p4)\n","        c5 = tf.keras.layers.BatchNormalization()(c5)\n","        c5 = tf.keras.layers.Dropout(0.3)(c5)#original 0.3\n","        c5 = tf.keras.layers.Conv2D(initial_filter*16, (3,3), activation=activation_func, kernel_initializer=initializer, padding='same', name='c5_2')(c5)\n","        c5 = tf.keras.layers.BatchNormalization()(c5)\n","\n","        #space latent\n","        u6 = tf.keras.layers.Conv2DTranspose(initial_filter*8, (2,2), strides=(2,2), padding='same', name='u6')(c5)\n","        u6 = tf.keras.layers.concatenate([u6, c4])\n","        c6 = tf.keras.layers.Conv2D(initial_filter*8, (3,3), activation=activation_func, kernel_initializer=initializer, padding='same', name='c6_1')(u6)\n","        c6 = tf.keras.layers.BatchNormalization()(c6)\n","        c6 = tf.keras.layers.Dropout(0.2)(c6) #original 0.2\n","        c6 = tf.keras.layers.Conv2D(initial_filter*8, (3,3), activation=activation_func, kernel_initializer=initializer, padding='same', name='c6_2')(c6)\n","        c6 = tf.keras.layers.BatchNormalization()(c6)\n","\n","        u7 = tf.keras.layers.Conv2DTranspose(initial_filter*4, (2,2), strides=(2,2), padding='same')(c6)\n","        u7 = tf.keras.layers.concatenate([u7, c3])\n","        c7 = tf.keras.layers.Conv2D(initial_filter*4, (3,3), activation=activation_func, kernel_initializer=initializer, padding='same')(u7)\n","        c7 = tf.keras.layers.BatchNormalization()(c7)\n","        c7 = tf.keras.layers.Dropout(0.2)(c7)# original 0.2\n","        c7 = tf.keras.layers.Conv2D(initial_filter*4, (3,3), activation=activation_func, kernel_initializer=initializer, padding='same')(c7)\n","        c7 = tf.keras.layers.BatchNormalization()(c7)\n","\n","        u8 = tf.keras.layers.Conv2DTranspose(initial_filter*2, (2,2), strides=(2,2), padding='same')(c7)\n","        u8 = tf.keras.layers.BatchNormalization()(u8)\n","        u8 = tf.keras.layers.concatenate([u8, c2])\n","        c8 = tf.keras.layers.Conv2D(initial_filter*2, (3,3), activation=activation_func, kernel_initializer=initializer, padding='same')(u8)\n","        c8 = tf.keras.layers.BatchNormalization()(c8)\n","        c8 = tf.keras.layers.Dropout(0.1)(c8) #original 0.1\n","        c8 = tf.keras.layers.Conv2D(initial_filter*2, (3,3), activation=activation_func, kernel_initializer=initializer, padding='same')(c8)\n","        c8 = tf.keras.layers.BatchNormalization()(c8)\n","\n","        u9 = tf.keras.layers.Conv2DTranspose(initial_filter, (2,2), strides=(2,2), padding='same')(c8)\n","        u9 = tf.keras.layers.BatchNormalization()(u9)\n","        u9 = tf.keras.layers.concatenate([u9, c1], axis=3)\n","        c9 = tf.keras.layers.Conv2D(initial_filter, (3,3), activation=activation_func, kernel_initializer=initializer, padding='same')(u9)\n","        c9 = tf.keras.layers.BatchNormalization()(c9)\n","        c9 = tf.keras.layers.Dropout(0.1)(c9)#original 0.1\n","        c9 = tf.keras.layers.Conv2D(initial_filter, (3,3), activation=activation_func, kernel_initializer=initializer, padding='same')(c9)\n","        c9 = tf.keras.layers.BatchNormalization()(c9)\n","\n","        outputs = tf.keras.layers.Conv2D(1, (1,1), activation='sigmoid')(c9)\n","        Model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n","        return Model\n","\n","    def train(self, X_train_, Y_train_, epochs, btz, X_test_ = None, Y_test_= None, min_lr = 0.001, max_lr = 0.8, show_history=True):\n","\n","        INIT_LR    = min_lr #1e-6  0.001\n","        MAX_LR     = max_lr   #1e-4\n","        steps_per_epoch = len(X_train_) // btz\n","        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","                                    filepath          = 'test',\n","                                    verbose           = True,\n","                                    save_weights_only = True,\n","                                    monitor           = 'val_loss',\n","                                    mode              = 'min',\n","                                    save_best_only=True)\n","        self.Model = self.Unet()\n","        self.Model.compile(optimizer=self.optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n","\n","        if X_test_ != None and Y_test_ != None:\n","          self.history = self.Model.fit(X_train_,Y_train_, validation_data=(X_test_, Y_test_), batch_size=btz,\n","                                 epochs = epochs, callbacks=[model_checkpoint_callback,CustomCallback(), printlearningrate()])\n","        else:\n","          self.history = self.Model.fit(X_train_,Y_train_, validation_split=0.20, batch_size=btz,\n","                                 epochs = epochs, callbacks=[model_checkpoint_callback,CustomCallback(), printlearningrate()])\n","        if show_history == True:\n","            print(self.history.history.keys())  # Ver las métricas disponibles en el historial\n","            # Graficar la pérdida del entrenamiento y la validación\n","            plt.plot(self.history.history['loss'], label='Pérdida de entrenamiento')\n","            plt.plot(self.history.history['val_loss'], label='Pérdida de validación')\n","            plt.xlabel('Epochs')\n","            plt.ylabel('Loss')\n","            plt.legend()\n","            plt.show()\n","\n","            # Graficar la precisión del entrenamiento y la validación\n","            plt.plot(self.history.history['accuracy'], label='accuracy training')\n","            plt.plot(self.history.history['val_accuracy'], label='accuracy validation')\n","            plt.xlabel('Epochs')\n","            plt.ylabel('Precision')\n","            plt.legend()\n","            plt.show()\n","\n","    def testing(self, X_test_, Y_test_, th = 0.5):\n","        metrics = []\n","        self.iou_score   = 0\n","        self.dc          = 0.0\n","        self.sensitivity = 0\n","        self.specificity = 0\n","        SM          = Semantic_loss_functions()\n","        #print(np.shape(Y_test_))\n","        for i in np.arange(len(X_test_)):\n","            img_  = resize(X_test_[i], (240, 240), mode='constant', preserve_range=True)\n","            if np.min(X_test_[i]) == np.max(X_test_[i]):\n","                continue\n","            img_ = np.expand_dims(img_, axis=0)\n","            y_p  = self.Model.predict(img_)\n","            im_p = (y_p>= th).astype(np.uint8)\n","            im_p = np.squeeze(im_p)\n","            y_test = np.uint8(Y_test_[i])\n","            intersection = np.logical_and( y_test, im_p)\n","            union = np.logical_or(y_test, im_p)\n","            #print('Interseccion',np.sum(intersection))\n","            if np.sum(union) > 0:\n","                self.iou_score += np.sum(intersection) / ( np.sum(union))\n","            else:\n","                self.iou_score += 0\n","            #print('np.sum(union)',np.sum(union))\n","\n","            im_p         = im_p.astype(np.float32)\n","            y_test       = y_test.astype(np.float32)\n","            self.dc          += tf.get_static_value(SM.dice_coef(y_test, im_p) )\n","            self.sensitivity += tf.get_static_value(SM.sensitivity(y_test, im_p ))\n","            self.specificity += tf.get_static_value(SM.specificity(y_test, im_p ))\n","\n","            #iou_score= iou_score/len(X_test_)\n","        self.IOU         = np.float32(self.iou_score/(len(X_test_)))\n","        self.dc          = np.float32( self.dc /(len(X_test_)))\n","        self.sensitivity = self.sensitivity / (len(X_test_))\n","        self.specificity =  self.specificity / (len(X_test_))\n","        #print('IOU score:', self.iou_score/(len(X_test_)))\n","        metrics.append(self.IOU)\n","        metrics.append(self.dc)\n","        metrics.append(self.sensitivity)\n","        metrics.append(self.specificity)\n","        return metrics\n","\n","    def get_model(self):\n","        return self.Model\n","\n","    def get_history(self):\n","        return self.history\n","\n","    def dice_coefficient(self, y_true, y_pred, smooth=1e-6):\n","        y_true_f = tf.keras.backend.flatten(y_true)\n","        y_pred_f = tf.keras.backend.flatten(y_pred)\n","        intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n","        return (2. * intersection + smooth) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + smooth)\n","\n","    def dice_loss(self, y_true, y_pred, smooth=1e-6):\n","        y_true = tf.cast(y_true, tf.float32)\n","        y_pred = tf.cast(y_pred, tf.float32)\n","\n","        # Asegúrate de que y_true y y_pred tienen la misma forma\n","        intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2])\n","        union = tf.reduce_sum(y_true, axis=[1, 2]) + tf.reduce_sum(y_pred, axis=[1, 2])\n","        dice = (2. * intersection + smooth) / (union + smooth)\n","\n","        # Devolvemos 1 menos la media del coeficiente Dice\n","        return 1 - tf.reduce_mean(dice)\n","\n","    def cross_entropy_loss(self, y_true, y_pred):\n","        return tf.keras.losses.binary_crossentropy(y_true, y_pred)\n","\n","    def combined_loss(self, y_true, y_pred):\n","        # Pérdida Dice\n","        dice_loss_value = self.dice_loss(y_true, y_pred)\n","\n","        # Pérdida de entropía cruzada binaria\n","        bce_loss_value = self.cross_entropy_loss(y_true, y_pred)\n","\n","        # Combinación ponderada de las pérdidas\n","        total_loss = 0.5 * dice_loss_value + 0.5 * bce_loss_value\n","        return total_loss"]},{"cell_type":"markdown","id":"0b878088","metadata":{"id":"0b878088"},"source":["<h1><center> UNET ++ </center></h1>"]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras import layers, models\n","from tensorflow.keras.layers import Input\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","class UNETPP:\n","    def __init__(self, input_shape=(240, 240, 1), num_filters=64, num_classes=1):\n","        self.input_shape = input_shape\n","        self.num_filters = num_filters\n","        self.num_classes = num_classes\n","\n","    def get_model(self):\n","        return self.model\n","\n","    def conv_block(self, input_tensor, num_filters):\n","        x = layers.Conv2D(num_filters, 3, padding='same')(input_tensor)\n","        x = layers.BatchNormalization()(x)\n","        x = layers.ReLU()(x)\n","        x = layers.Conv2D(num_filters, 3, padding='same')(x)\n","        x = layers.BatchNormalization()(x)\n","        x = layers.ReLU()(x)\n","        return x\n","\n","    def dense_concat(self, blocks):\n","        return layers.Concatenate()(blocks)\n","\n","    def build(self):\n","        inputs = Input(shape=self.input_shape)\n","\n","        # Encoder\n","        c1 = self.conv_block(inputs, self.num_filters)\n","        p1 = layers.MaxPooling2D((2, 2))(c1)\n","\n","        c2 = self.conv_block(p1, self.num_filters * 2)\n","        p2 = layers.MaxPooling2D((2, 2))(c2)\n","\n","        c3 = self.conv_block(p2, self.num_filters * 4)\n","        p3 = layers.MaxPooling2D((2, 2))(c3)\n","\n","        c4 = self.conv_block(p3, self.num_filters * 8)\n","        p4 = layers.MaxPooling2D((2, 2))(c4)\n","\n","        # Bridge\n","        bridge = self.conv_block(p4, self.num_filters * 16)\n","\n","        # Decoder with dense connections\n","        u4 = layers.UpSampling2D((2, 2))(bridge)\n","        u4 = self.dense_concat([u4, c4])\n","        c4d = self.conv_block(u4, self.num_filters * 8)\n","\n","        u3 = layers.UpSampling2D((2, 2))(c4d)\n","        u3 = self.dense_concat([u3, c3])\n","        c3d = self.conv_block(u3, self.num_filters * 4)\n","\n","        u2 = layers.UpSampling2D((2, 2))(c3d)\n","        u2 = self.dense_concat([u2, c2])\n","        c2d = self.conv_block(u2, self.num_filters * 2)\n","\n","        u1 = layers.UpSampling2D((2, 2))(c2d)\n","        u1 = self.dense_concat([u1, c1])\n","        c1d = self.conv_block(u1, self.num_filters)\n","\n","        outputs = layers.Conv2D(self.num_classes, 1, activation='sigmoid')(c1d)\n","\n","        model = models.Model(inputs=[inputs], outputs=[outputs])\n","        return model\n","\n","    def train(self, X_train, Y_train, batch_size, epochs, optimizer='adam', X_test=None, Y_test=None):\n","        self.model = self.build()\n","        self.model.summary()\n","        self.model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n","        if X_test is not None and Y_test is not None:\n","            self.history = self.model.fit(X_train, Y_train, validation_data=(X_test, Y_test),\n","                                    batch_size=batch_size, epochs=epochs)\n","        else:\n","            self.history = self.model.fit(X_train, Y_train, validation_split=0.2, batch_size=batch_size, epochs=epochs)\n","        return self.model\n","\n","    def testing(self, X_test, Y_test, th=0.5, show_images=False):\n","        metrics = []\n","        iou_score = 0\n","        dc = 0.0\n","        sensitivity = 0\n","        specificity = 0\n","        SM = Semantic_loss_functions()\n","\n","        for i in np.arange(len(X_test)):\n","            img_ = resize(X_test[i], (240, 240), mode='constant', preserve_range=True)\n","            if np.min(X_test[i]) == np.max(X_test[i]):\n","                continue\n","            img_ = np.expand_dims(img_, axis=0)\n","            y_p = self.model.predict(img_)\n","            im_p = (y_p >= th).astype(np.uint8)\n","            im_p = np.squeeze(im_p)\n","            y_test = np.uint8(Y_test[i])\n","            if show_images:\n","                plt.subplot(1, 2, 1), plt.imshow(im_p, cmap='gray')\n","                plt.subplot(1, 2, 2), plt.imshow(y_test, cmap='gray')\n","                plt.show()\n","            intersection = np.logical_and(y_test, im_p)\n","            union = np.logical_or(y_test, im_p)\n","            if np.sum(union) > 0:\n","                iou_score += np.sum(intersection) / np.sum(union)\n","            else:\n","                iou_score += 0\n","            im_p = im_p.astype(np.float32)\n","            y_test = y_test.astype(np.float32)\n","            dc += tf.get_static_value(SM.dice_coef(y_test, im_p))\n","            sensitivity += tf.get_static_value(SM.sensitivity(y_test, im_p))\n","            specificity += tf.get_static_value(SM.specificity(y_test, im_p))\n","\n","        IOU = np.float32(iou_score / len(X_test))\n","        dc = np.float32(dc / len(X_test))\n","        sensitivity = sensitivity / len(X_test)\n","        specificity = specificity / len(X_test)\n","        metrics.append(IOU)\n","        metrics.append(dc)\n","        metrics.append(sensitivity)\n","        metrics.append(specificity)\n","        return metrics\n","    def get_model(self):\n","        return self.model\n","    def get_history(self):\n","        return self.history"],"metadata":{"id":"Stpa4JObnbBy"},"id":"Stpa4JObnbBy","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"672bfd25","metadata":{"id":"672bfd25"},"outputs":[],"source":["#https://github.com/shruti-jadon/Semantic-Segmentation-Loss-Functions\n","import tensorflow as tf\n","import tensorflow.keras.backend as K\n","#from keras.losses import binary_crossentropy\n","\n","beta = 0.25\n","alpha = 0.25\n","gamma = 2\n","epsilon = 1e-5\n","smooth = 1\n","\n","\n","class Semantic_loss_functions(object):\n","    def __init__(self):\n","        print (\"semantic loss functions initialized\")\n","\n","    def dice_coef(self, y_true, y_pred):\n","        #intersection = np.sum(y_true * y_pred)\n","        #return (2. * intersection) / (np.sum(y_true) + np.sum(y_pred))\n","        y_true_f = K.flatten(y_true)\n","        y_pred_f = K.flatten(y_pred)\n","        intersection = K.sum(y_true_f * y_pred_f)\n","        #print('K epsilon',K.epsilon())\n","        return (2. * intersection + 1) / (\n","                    K.sum(y_true_f) + K.sum(y_pred_f) + epsilon)# 1)\n","\n","    def sensitivity(self, y_true, y_pred):\n","        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","        return true_positives / (possible_positives + K.epsilon())\n","\n","    def specificity(self, y_true, y_pred):\n","        true_negatives = K.sum(\n","            K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n","        possible_negatives = K.sum(K.round(K.clip(1 - y_true, 0, 1)))\n","        return true_negatives / (possible_negatives + K.epsilon())"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}